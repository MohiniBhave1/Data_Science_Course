{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset,mnist_info = tfds.load(name='mnist',with_info=True,as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>We manually spilt into validation and train dataset</li>\n",
    "<li>We can either count the # train samples, or use the mnist_info</li>\n",
    "<li>Normally we'd like to scale our data in some way to make the result more numerically stable ( inputs b/w 0 and 1 )</li>\n",
    "<li>MNIST dataset has images value between 0 and 255 . So if we divide by 255 then we will get values between 0 and 1</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train , mnist_test = mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples,tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['train'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples , tf.int64)\n",
    "\n",
    "def scale( image,label ):\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image /= 255.\n",
    "    return image,label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale) \n",
    "\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Shuffling = keeping the same information but in a different order</li>\n",
    "<li>Buffer size used when dealing with enormous datasets. we can't shuffle all at once</li> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Batch size = 1 = SGD</li>\n",
    "<li>Batch size = # samples = (single batch)GD</li>\n",
    "<li>1 < Batch size <#samples = mini-batch GD</li>\n",
    "<li>When batching we find the average loss and average accuracy</li>\n",
    "<li>The model expects the validation dataset in batch form too</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(mnist_info.splits['test'].num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>The MNIST data is iterable and in 2-tuple format ( as_supervised = True ) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs , validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>The underlying assumption is that all hidden layers are of the same size</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 150\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the optimizer we'd like to use, \n",
    "# the loss function, \n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "540/540 - 7s - loss: 0.2875 - accuracy: 0.9164 - val_loss: 0.1409 - val_accuracy: 0.9578\n",
      "Epoch 2/100\n",
      "540/540 - 5s - loss: 0.1137 - accuracy: 0.9653 - val_loss: 0.0889 - val_accuracy: 0.9738\n",
      "Epoch 3/100\n",
      "540/540 - 5s - loss: 0.0765 - accuracy: 0.9765 - val_loss: 0.0762 - val_accuracy: 0.9765\n",
      "Epoch 4/100\n",
      "540/540 - 5s - loss: 0.0571 - accuracy: 0.9825 - val_loss: 0.0714 - val_accuracy: 0.9778\n",
      "Epoch 5/100\n",
      "540/540 - 5s - loss: 0.0472 - accuracy: 0.9846 - val_loss: 0.0579 - val_accuracy: 0.9832\n",
      "Epoch 6/100\n",
      "540/540 - 5s - loss: 0.0369 - accuracy: 0.9883 - val_loss: 0.0619 - val_accuracy: 0.9802\n",
      "Epoch 7/100\n",
      "540/540 - 5s - loss: 0.0297 - accuracy: 0.9903 - val_loss: 0.0355 - val_accuracy: 0.9883\n",
      "Epoch 8/100\n",
      "540/540 - 5s - loss: 0.0286 - accuracy: 0.9911 - val_loss: 0.0262 - val_accuracy: 0.9918\n",
      "Epoch 9/100\n",
      "540/540 - 5s - loss: 0.0218 - accuracy: 0.9929 - val_loss: 0.0294 - val_accuracy: 0.9895\n",
      "Epoch 10/100\n",
      "540/540 - 5s - loss: 0.0209 - accuracy: 0.9929 - val_loss: 0.0305 - val_accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b8620fee88>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "model.fit(train_data,epochs=NUM_EPOCHS,validation_data=(validation_inputs,validation_targets),verbose=2,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss didn't change too much. Even after the first epoch 540 different weights and bias updates. The accuracy shows what % of the cases the output were equal to the targets. We usually keep an eye on the validation loss ( or set early stopping mechnisms ) to determine whether the model is overfitting )\n",
    "VALIDATION ACCURACY = TRUE ACCURACY OF THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After changing the hidden layer size to 100 :\n",
    "Drastically increased the accuracy of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Width : 200\n",
    "Find the variable: \"hidden_layer_size\" and change it to 200.\n",
    "\n",
    "The validation accuracy is significantly higher (as the algorithm with 50 hidden units was too simple of a model).\n",
    "\n",
    "Naturally, it takes the algorithm much longer to train (unless early stopping is triggered too soon).( 5 to 6s compared to 4s for 100 )\n",
    "\n",
    "A hidden layer size of 500 (and not only) works even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Depth : add another hidden layer :\n",
    "We can see that the accuracy of the model does not necessarily improve. This is an important lesson for us. Fiddling with a single hyperparameter may not be enough. Sometimes, a deeper net needs to also be wider in order to have higher accuracy. Maybe you need more epochs?\n",
    "\n",
    "**ADDITIONAL TASK: Try this new model, but with a wider one (200-500 hidden units). Basically, combine this and the previous exercises**\n",
    "\n",
    "In any case, it takes longer for the algorithm to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Width and depth : 5 hidden layer and 1000 size : \n",
    "The result (as you can see below) is that our model's training was going very well, until it overfit. It did so by quite a lot.\n",
    "\n",
    "It took my personal computer around 5-6 minutes to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fiddle with activation functions :\n",
    "ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'.\n",
    "Analogically to the previous lecture, we can change the activation functions. This time though, we will use different activators for the different layers.\n",
    "\n",
    "The result should not be significantly different. However, with different width and depth, that may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch size :\n",
    "A bigger batch size results in slower training. That's what we expected from the theory. We are taking advantage of batching because of the amazing speed increase.\n",
    "\n",
    "Notice that the validation accuracy starts from a low number and with 5 epochs actually **finishes** at a lower number. That's because there are **fewer** updates in a single epoch.\n",
    "\n",
    "*Try a batch size of 30,000 or 50,000. That's very close to single batch GD for this problem. What do you think about the speed?You will need to change the max epochs to 100 (for instance), as 5 epochs won't be enough to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch size = 1 :\n",
    "A batch size of 1 results in the SGD. It takes the algorithm very little time to process a single batch (as it is one data point), but there are thousands of batches (54000 to be precise), thus the algorithm is actually slow. Remember that this depends on the number of cores that you train on. If you are using a CPU with 4 or 8 cores, you can only train 4 or 8 batches at once. The middle ground (mini-batching such as 100 samples per batch) is optimal.\n",
    "\n",
    "Notice that the validation accuracy starts from a high number. That's because there are **lots** updates in a single epoch. Once the training is over, the accuracy is lower than all other batch sizes (SGD was an approximation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjust learning rate :\n",
    "We create the custom optimizer with:\n",
    "\n",
    "    custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "Then we change the respective argument in model.compile to reflect this: \n",
    "\n",
    "    model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "Since the learning rate is lower than normal, we may need to adjust the max_epochs (to, say, 50). \n",
    "\n",
    "The result is basically the same, but we reach it much slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjust learning rate :  0.02 :\n",
    "\n",
    "While Adam adapts to the problem, if the orders of magnitude are too different, it may not have time to adjust accordingly. We start overfitting before we can reach a neat solution.\n",
    "\n",
    "Therefore, for this problem, even 0.02 is a **HIGH** starting learning rate. What if you try a learning rate of = 1?\n",
    "\n",
    "It's a good practice to try 0.001, 0.0001, and 0.00001. If it makes no difference, pick whatever, otherwise it makes sense to fiddle with the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0909 - accuracy: 0.9784\n"
     ]
    }
   ],
   "source": [
    "test_loss , test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.09. Test accuracy : 97.84%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss : {0:.2f}. Test accuracy : {1:.2f}%'.format(test_loss,test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
